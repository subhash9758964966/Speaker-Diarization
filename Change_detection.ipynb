{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Change_detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+tCX9se7aEKcIdm/7lTho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muskang48/Speaker-Diarization/blob/master/Change_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loz5xZXEO-pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "file_list = ['Hindi1_01', 'Hindi1_02', 'Hindi1_03']\n",
        "\n",
        "\n",
        "\n",
        "def extract_feature(file_name):\n",
        "    file = \"/content/drive/My Drive/SRU/\" + file_name + \".wav\"\n",
        "    frame_size = 2048\n",
        "    frame_shift = 512\n",
        "    y, sr = librosa.load(file)\n",
        "    #MFCC Extraction \n",
        "    mfccs = librosa.feature.mfcc(y, sr, n_mfcc=12, hop_length=frame_shift, n_fft=frame_size)\n",
        "    mfcc_delta = librosa.feature.delta(mfccs)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "    mfcc = mfccs[1:, ]\n",
        "    norm_mfcc = (mfcc - np.mean(mfcc, axis=1, keepdims=True)) / np.std(mfcc, axis=1, keepdims=True)\n",
        "    norm_mfcc_delta = (mfcc_delta - np.mean(mfcc_delta, axis=1, keepdims=True)) / np.std(mfcc_delta, axis=1, keepdims=True)\n",
        "    norm_mfcc_delta2= (mfcc_delta2 - np.mean(mfcc_delta2, axis=1, keepdims=True)) / np.std(mfcc_delta2, axis=1, keepdims=True)\n",
        "\n",
        "    ac_feature = np.vstack((norm_mfcc, norm_mfcc_delta, norm_mfcc_delta2))\n",
        "   \n",
        " #Loading Annotation File\n",
        "    ann = pd.read_csv('/content/drive/My Drive/SRU/annotations1 (1).csv')\n",
        "    ann['End_point'] = ann['Duration'] + ann['Offset']\n",
        "\n",
        "    change_point = []\n",
        "    for i in range(len(ann['End_point'])):\n",
        "        dur_1 = int((ann['End_point'][i]-0.075)*sr)  # left 50ms\n",
        "        dur_2 = int((ann['End_point'][i]+0.075)*sr)  # right 50ms\n",
        "        change_point.append((dur_1, dur_2))\n",
        "   \n",
        "    sub_seq_len = int(3.2*sr/frame_shift)\n",
        "    sub_seq_step= int(0.8*sr/frame_shift)\n",
        "\n",
        "    feature_len = ac_feature.shape[1]\n",
        "\n",
        "    def is_change_point(n):\n",
        "        flag = False\n",
        "        for x in change_point:\n",
        "            if n > x[0] and n < x[1]:\n",
        "                flag = True\n",
        "                break\n",
        "\n",
        "            if n+frame_size-1 > x[0] and n+frame_size-1 < x[1]:\n",
        "                flag = True\n",
        "                break\n",
        "        return flag\n",
        "\n",
        "    sub_train_x = []\n",
        "    sub_train_y = []\n",
        "    for i in range(0, feature_len-sub_seq_len, sub_seq_step):\n",
        "        sub_seq_x = np.transpose(ac_feature[:, i: i+sub_seq_len])\n",
        "        sub_train_x.append(sub_seq_x[np.newaxis, :, :])\n",
        "        tmp = []\n",
        "        for index in range(i, i+sub_seq_len):\n",
        "            if is_change_point(index*frame_shift):\n",
        "                tmp.append(1)\n",
        "            else:\n",
        "                tmp.append(0)\n",
        "        lab_y = np.array(tmp)\n",
        "        lab_y = np.reshape(lab_y, (1, sub_seq_len))\n",
        "        sub_train_y.append(lab_y)\n",
        "    return sub_train_x, sub_train_y\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "    for audio_file in file_list:\n",
        "        new_train_x, new_train_y = extract_feature(audio_file)\n",
        "        new_train_x = np.vstack(new_train_x)\n",
        "        new_train_y = np.vstack(new_train_y)\n",
        "        print(new_train_x.shape)\n",
        "        print(new_train_y.shape)\n",
        "\n",
        "        all_x.append(new_train_x)\n",
        "        all_y.append(new_train_y)\n",
        "    print(len(all_x))\n",
        "    print(len(all_y))\n",
        "\n",
        "    all_x_stack = np.vstack(all_x)\n",
        "    all_y_stack = np.vstack(all_y)\n",
        "    print(all_x_stack.shape, all_y_stack.shape)\n",
        "    print('over')\n",
        "    return all_x_stack, all_y_stack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L89MdqcSppe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18c1220e-d656-40d9-ecca-442dfff53d52"
      },
      "source": [
        "from keras.legacy import interfaces\n",
        "from keras.optimizers import Optimizer\n",
        "from keras import backend as K\n",
        "#SNORM Optimizer\n",
        "class SMORMS3(Optimizer):\n",
        "    \"\"\"SMORMS3 optimizer.\n",
        "    Default parameters follow those provided in the blog post.\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        epsilon: float >= 0. Fuzz factor.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "    # References\n",
        "        - [RMSprop loses to SMORMS3 - Beware the Epsilon!](http://sifter.org/~simon/journal/20150420.html)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, epsilon=1e-16, decay=0.,\n",
        "                 **kwargs):\n",
        "        super(SMORMS3, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_decay = decay\n",
        "\n",
        "    @interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        shapes = [K.shape(p) for p in params]\n",
        "        ms = [K.zeros(shape) for shape in shapes]\n",
        "        vs = [K.zeros(shape) for shape in shapes]\n",
        "        mems = [K.zeros(shape) for shape in shapes]\n",
        "        self.weights = [self.iterations] + ms + vs + mems\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        learning_rate = self.learning_rate\n",
        "        if self.initial_decay > 0:\n",
        "            learning_rate *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "\n",
        "\n",
        "        for p, g, m, v, mem in zip(params, grads, ms, vs, mems):\n",
        "\n",
        "            r = 1. / (1. + mem)\n",
        "            new_m = (1. - r) * m + r * g\n",
        "            new_v = (1. - r) * v + r * K.square(g)\n",
        "            denoise = K.square(new_m) / (new_v + self.epsilon)\n",
        "            new_p = p - g * K.minimum(learning_rate, denoise) / (K.sqrt(new_v) + self.epsilon)\n",
        "            new_mem = 1. + mem * (1. - denoise)\n",
        "\n",
        "            self.updates.append(K.update(m, new_m))\n",
        "            self.updates.append(K.update(v, new_v))\n",
        "            self.updates.append(K.update(mem, new_mem))\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'epsilon': self.epsilon}\n",
        "        base_config = super(SMORMS3, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BqHXUL4pnww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.core import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, TimeDistributed, Dropout\n",
        "from keras.layers import LSTM\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "def train_bilstm():\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(TimeDistributed(Dense(32)))\n",
        "    model.add(TimeDistributed(Dense(32)))\n",
        "    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
        "\n",
        "    model.build(input_shape=(None, 137, 35))\n",
        "\n",
        "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=SMORMS3(), metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    all_x, all_y = load_dataset()\n",
        "    print(all_y.shape, np.sum(all_y))\n",
        "\n",
        "    subsample_all_x = []\n",
        "    subsample_all_y = []\n",
        "    for index in range(all_y.shape[0]):\n",
        "        class_positive = sum(all_y[index])\n",
        "        if class_positive > 5:\n",
        "            subsample_all_x.append(all_x[index][np.newaxis, :, :])\n",
        "            subsample_all_y.append(all_y[index])\n",
        "\n",
        "    all_x = np.vstack(subsample_all_x)\n",
        "    all_y = np.vstack(subsample_all_y)\n",
        "    print(all_y.shape, np.sum(all_y))\n",
        "\n",
        "    all_y = all_y[:, :, np.newaxis]\n",
        "\n",
        "    indices = np.random.permutation(all_x.shape[0])\n",
        "    all_x_random = all_x[indices]\n",
        "    all_y_random = all_y[indices]\n",
        "\n",
        "    datasize = all_x_random.shape[0]\n",
        "    train_size = int(datasize*0.97)\n",
        "    train_x = all_x_random[0:train_size]\n",
        "    valid_x = all_x_random[train_size:]\n",
        "\n",
        "    train_y = all_y_random[0:train_size]\n",
        "    valid_y = all_y_random[train_size:]\n",
        "    print('train over')\n",
        "\n",
        "    my = model.fit(x=train_x, y=train_y, batch_size=256, epochs=50,\n",
        "              validation_data=(valid_x, valid_y), shuffle=True)\n",
        "    model.save('/content/drive/My Drive/SRU/model_hindi_2.h5')\n",
        "    def save_model(model, json_model_file, h5_model_file):\n",
        "        # serialize model to JSON\n",
        "        model_json = model.to_json()\n",
        "        with open(json_model_file, \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        # serialize weights to HDF5\n",
        "        model.save_weights(h5_model_file)\n",
        "        print(\"Saved model to disk\")\n",
        "\n",
        "    model_name = 'speech_seg1'\n",
        "    json_model_file = '/content/drive/My Drive/SRU/model_hindi_2'+'.json'\n",
        "    h5_model_file = '/content/drive/My Drive/SRU/model_hindi_2'+'.h5'\n",
        "    save_model(model, json_model_file, h5_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n1h9aJ4U2Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calling the function to Train the Chnage detction Model based on Bi-LSTM\n",
        "train_bilstm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}